{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在李沐的实现中，DecoderBlock的call的X、state[2]、运算方式\n",
    "train: \n",
    "- X是序列全部，一次性处理；\n",
    "- state[2]初始化为None，因为只运行一次，所以不会运行到key_values = tf.concat((state[2][self.i], X), axis=1)去拼接上次运行到隐向量\n",
    "- 运算方式：一次性算出每个时间步的预测值\n",
    "predict：\n",
    "- X是上一次的预测值，第一次是起始标志符\n",
    "- state[2]初始化为None，每次运行将把X和上次的state[2]拼接起来作为key_values并更新state[2]\n",
    "- 运算方式：第一个词是起始标志符，逐个词计算，直到结束\n",
    "对比第一个多头的行为：\n",
    "train时Q、K、V都是序列；predict时Q是上个词，K、V是key_values（上个词和之前的decoder block隐向量的拼接）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(tf.keras.layers.Layer):\n",
    "    \"\"\"解码器中第i个块\"\"\"\n",
    "    def __init__(self, key_size, query_size, value_size, num_hiddens,\n",
    "                 norm_shape, ffn_num_hiddens, num_heads, dropout, i, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.i = i\n",
    "        self.attention1 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm1 = AddNorm(norm_shape, dropout)\n",
    "        self.attention2 = d2l.MultiHeadAttention(key_size, query_size, value_size, num_hiddens, num_heads, dropout)\n",
    "        self.addnorm2 = AddNorm(norm_shape, dropout)\n",
    "        self.ffn = PositionWiseFFN(ffn_num_hiddens, num_hiddens)\n",
    "        self.addnorm3 = AddNorm(norm_shape, dropout)\n",
    "\n",
    "    def call(self, X, state, **kwargs):\n",
    "        enc_outputs, enc_valid_lens = state[0], state[1]\n",
    "        # 训练阶段，输出序列的所有词元都在同一时间处理，\n",
    "        # 因此state[2][self.i]初始化为None。\n",
    "        # 预测阶段，输出序列是通过词元一个接着一个解码的，\n",
    "        # 因此state[2][self.i]包含着直到当前时间步第i个块解码的输出表示\n",
    "        if state[2][self.i] is None:\n",
    "            key_values = X\n",
    "        else:\n",
    "            key_values = tf.concat((state[2][self.i], X), axis=1)\n",
    "        state[2][self.i] = key_values\n",
    "        if kwargs[\"training\"]:\n",
    "            batch_size, num_steps, _ = X.shape\n",
    "           # dec_valid_lens的开头:(batch_size,num_steps),\n",
    "            # 其中每一行是[1,2,...,num_steps]\n",
    "            dec_valid_lens = tf.repeat(tf.reshape(tf.range(1, num_steps + 1),\n",
    "                                                 shape=(-1, num_steps)), repeats=batch_size, axis=0)\n",
    "\n",
    "        else:\n",
    "            dec_valid_lens = None\n",
    "\n",
    "        # 自注意力\n",
    "        X2 = self.attention1(X, key_values, key_values, dec_valid_lens, **kwargs)\n",
    "        Y = self.addnorm1(X, X2, **kwargs)\n",
    "        # 编码器－解码器注意力。\n",
    "        # enc_outputs的开头:(batch_size,num_steps,num_hiddens)\n",
    "        Y2 = self.attention2(Y, enc_outputs, enc_outputs, enc_valid_lens, **kwargs)\n",
    "        Z = self.addnorm2(Y, Y2, **kwargs)\n",
    "        return self.addnorm3(Z, self.ffn(Z), **kwargs), state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(d2l.AttentionDecoder):\n",
    "    def __init__(self, vocab_size, key_size, query_size, value_size,\n",
    "                 num_hiddens, norm_shape, ffn_num_hidens, num_heads, num_layers, dropout, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.num_layers = num_layers\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, num_hiddens)\n",
    "        self.pos_encoding = d2l.PositionalEncoding(num_hiddens, dropout)\n",
    "        self.blks = [DecoderBlock(key_size, query_size, value_size, num_hiddens, norm_shape,\n",
    "                                  ffn_num_hiddens, num_heads, dropout, i) for i in range(num_layers)]\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def init_state(self, enc_outputs, enc_valid_lens, *args):\n",
    "        return [enc_outputs, enc_valid_lens, [None] * self.num_layers]\n",
    "\n",
    "    def call(self, X, state, **kwargs):\n",
    "        X = self.pos_encoding(self.embedding(X) * tf.math.sqrt(tf.cast(self.num_hiddens, dtype=tf.float32)), **kwargs)\n",
    "        self._attention_weights = [[None] * len(self.blks) for _ in range(2)]  # 解码器中2个注意力层\n",
    "        for i, blk in enumerate(self.blks):\n",
    "            X, state = blk(X, state, **kwargs)\n",
    "            # 解码器自注意力权重\n",
    "            self._attention_weights[0][i] = blk.attention1.attention.attention_weights\n",
    "            # “编码器－解码器”自注意力权重\n",
    "            self._attention_weights[1][i] = blk.attention2.attention.attention_weights\n",
    "        return self.dense(X), state\n",
    "\n",
    "    @property\n",
    "    def attention_weights(self):\n",
    "        return self._attention_weights"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
