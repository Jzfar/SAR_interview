{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 为什么缩放点积attention要除以根号d，为什么是根号d？\n",
    "当q、k维度较高时点积结果可能较大，引起softmax梯度消失，所以除以根号d缩放；\n",
    "q、k的点积方差为d，除以根号d能将点积方差稳定到1，有利于模型训练的稳定性。\n",
    "1.1 为什么q、k的点积方差是d\n",
    "假设q、k是独立的并且每维符合均值0方差1的分布，q、k的点积是均值0方差d\n",
    "1.2 为什么点积结果大softmax梯度消失\n",
    "如果softmax的输入存在某个过大的值，softmax的结果会是one-hot（因为指数函数斜率随着值增大而增大，使得x轴上的小变化引起y轴的大变化）\n",
    "softmax对每个输入求导1）i=j，y'=pi * (1-pi) 2)i!=j, y'= -pi * pj 都会是接近零的数\n",
    "1.3 为什么点积能衡量向量的相似性\n",
    "归一化后的两个向量a、b的欧式距离的平方与a、b点积负相关：d(a,b)^2 = 1 - 2ab\n",
    "\n",
    "2. 为什么要多头，不单头\n",
    "多个子空间信息，每个头的参数随机初始化并且各自独立学习，自然会学到不同的注意力分布\n",
    "self attention会让每个位置的词学到和自身最大的注意力，多头能获取多种注意力的分配方式\n",
    "原本的高维空间向量的分布稀疏，而attention通过点击计算相似性，高维空间不利于模型学习向量间的相似性，分割多头让原本高维空间分割为多个低维空间\n",
    "从transformer的实验结果看，不是头越多越好或者越少越好\n",
    "\n",
    "3. 为什么qkv要经过原始词向量分别用三个线性变换的到？不用原始的词向量\n",
    "如果不经单独的线性变换，attention score矩阵会是对称矩阵，qk经过不同的投影增强attention score矩阵的泛化能力\n",
    "如果不经投影，每个词互相的注意力分数一样。“我是一个男孩”中“男孩”对“我”对修饰性应该大雨“我”对“男孩”的修饰性。\n",
    "\n",
    "4. 为什么attention score用缩放点积，不用加法（MLP）？\n",
    "计算复杂度低，加法的隐藏层数一般是d层，复杂度为O(nndd)\n",
    "在维度低时点积和加法表现差不多，在dk升高时加法注意力的表现优势更明显\n",
    "\n",
    "5. 在计算attention时，如何对padding做mask\n",
    "在要mask的位置，对点积的结果加一个负无穷\n",
    "\n",
    "6. 训练和预测时，decoder的输入\n",
    "训练：第一个self attention的QKV一样，是整个序列，在算注意力矩阵时，用mask矩阵只激活这个词之前的注意力\n",
    "推理：第一个self attention的Q是上一个推理出来的词，K、V是已经预测出来的序列\n",
    "\n",
    "7. 残差连接的作用\n",
    "残差连接是让输入层跳过中间多层，直接加在输出层的方法。这样缩短输入层到输出层的路径，让输出层的梯度能直达输入层，缓解深层网络带来梯度消失的问题，同时加快模型的收敛；另一方面增强模型的表达能力，因为输出层能直接获取原始信息，避免原始信息在多层非线性变换中丢失。\n",
    "\n",
    "8. 层归一化的作用，为什么不用批量归一化\n",
    "归一化能稳定数值分布，加快模型收敛。\n",
    "transformer的输入是可变长度的序列，如果使用批量归一化，padding会对同一时间步真实的词向量造成干扰。\n",
    "使用层归一化，对每个样本在序列内的每个词向量进行归一化，减少了内部协变量偏移，加速收敛，缓解梯度爆炸或者梯度消失的问题，层归一化引入可学习的缩放和平移参数，能灵活调整归一化后数据的分布，增强表达能力。\n",
    "\n",
    "    8.1 什么是内部协变量偏移？内部协变量偏移是深度神经网络的训练中，每一层输入的分布会随着前一层参数的变化而变化，这种分布的不稳定性会导致训练\n",
    "\n",
    "    8.2 为什么层归一化能缓解梯度爆炸或梯度消失问题？\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/82391768\n",
    "\n",
    "\n",
    "https://www.nowcoder.com/discuss/387725948110602240"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "证明点积的方差是维度数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "arr1=np.random.normal(size=(3,1000))\n",
    "arr2=np.random.normal(size=(3,1000))\n",
    "result=np.dot(arr1.T,arr2) # 这里是 arr1.T\n",
    "arr_var=np.var(result)\n",
    "print(arr_var)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "演示对padding的mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4,), dtype=int32, numpy=array([2, 3, 4, 5], dtype=int32)>,\n",
       " TensorShape([4, 8, 8]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "seq_len = 8\n",
    "logits = tf.random.normal([batch_size, seq_len, seq_len])\n",
    "valid_len = tf.range(start=2, limit=6)\n",
    "valid_len, logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[ True  True False False False False False False]\n",
      " [ True  True False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False False]\n",
      " [False False False False False False False False]], shape=(8, 8), dtype=bool)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 8, 8), dtype=float32, numpy=\n",
       "array([[[ 5.3475684e-01, -2.2182615e+00, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-1.5040651e-01,  1.1526464e+00, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10]],\n",
       "\n",
       "       [[ 2.2846675e-01, -2.0781328e-01, -1.0022382e+00, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-3.5660267e-01,  1.6159245e-01, -2.1226290e-01, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [ 2.1376680e-01,  1.8086412e+00, -1.2659311e-01, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10]],\n",
       "\n",
       "       [[-1.6969670e+00,  1.0759628e+00,  5.7221115e-01, -9.4396573e-01,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-2.2482439e-03,  1.0728773e+00, -2.3766153e-01, -2.7095990e+00,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-1.4692513e+00,  1.2535967e+00,  1.7930207e+00,  6.8499547e-01,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [ 5.7656097e-01,  3.9538700e-02,  9.5010817e-01, -2.5113299e-01,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10]],\n",
       "\n",
       "       [[-5.4658896e-01,  5.8327740e-01,  1.5871593e+00, -1.4625923e-01,\n",
       "          1.9910447e-01, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [ 7.9888451e-01, -1.4633336e+00, -3.0266911e-01,  1.5552215e-01,\n",
       "          4.9658930e-01, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-4.1394493e-01, -2.2495012e+00,  1.9463279e+00,  1.2972437e+00,\n",
       "         -6.9760138e-01, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-8.8589382e-01, -9.2385405e-01,  8.1439260e-03, -2.6061604e+00,\n",
       "         -4.3891695e-01, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [ 2.1873896e-01,  8.9638156e-01,  9.4684243e-01, -1.9292119e+00,\n",
       "          4.4583166e-01, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10],\n",
       "        [-9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10,\n",
       "         -9.9999997e-10, -9.9999997e-10, -9.9999997e-10, -9.9999997e-10]]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_mask(logits, valid_len):\n",
    "    range_matrix = tf.range(seq_len)[None, :] # 1, seq_len\n",
    "    valid_len = valid_len[:, None] # batchsize, 1\n",
    "    mask = range_matrix < valid_len # batch_size, seq_len\n",
    "    mask = mask[:, tf.newaxis, :]\n",
    "    mask = tf.logical_and(mask, tf.transpose(mask, perm=[0,2,1]))\n",
    "    print(mask[0])\n",
    "    mask = tf.cast(mask, tf.float32)\n",
    "    masked_logits = logits * mask + (1-mask)*-1e-9\n",
    "    return masked_logits\n",
    "softmax_mask(logits=logits, valid_len=valid_len)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
