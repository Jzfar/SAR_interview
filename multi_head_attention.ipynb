{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dense是对全连阶层专门对实现（其他的还有LSTM、Conv等），继承自Layer\n",
    "dense表示全连阶层的权重矩阵是“密集的”——输入层、输出层的每个神经元相连（而卷基层是稀疏的，输出层的神经元只和“感受野”内的输入层神经元相连）\n",
    "另外全连阶层在学术上命名为fully connected layer/ dense layer，用dense简洁命名了全连阶层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([32, 10, 512]),\n",
       " TensorShape([32, 20, 512]),\n",
       " TensorShape([32, 20, 512]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 32\n",
    "q_seq_len = 10\n",
    "kv_seq_len = 20\n",
    "d_model = 512 # 这里q、k、v的维度一样\n",
    "num_heads = 8\n",
    "\n",
    "q = tf.random.normal([batch_size, q_seq_len, d_model])\n",
    "k = tf.random.normal([batch_size, kv_seq_len, d_model])\n",
    "v = tf.random.normal([batch_size, kv_seq_len, d_model])\n",
    "q.shape, k.shape, v.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaled_attention_logits命名原理：\n",
    "scaled_attention表示 QK/sqrt(dk)这一数学过程\n",
    "在机器学习中，未经过softmax归一化的分数是logits，softmax将logits转为概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(q, k, v):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True) \n",
    "    # 转置K，对齐q_dim和k_dim， matmul_qk的每行是每个queyr和「序列中」每个key的点积\n",
    "    sqrt_dk = tf.math.sqrt(tf.cast(tf.shape(k)[-1], tf.float32))\n",
    "    scaled_attention_logits = matmul_qk / sqrt_dk\n",
    "    weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "    output = tf.matmul(weights, v)\n",
    "    return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class 子类名(父类名):\n",
    "    def __init__(self):\n",
    "        super(子类名, self).__init__()\n",
    "'''\n",
    "class MultiHeadAttention(Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0\n",
    "        self.depth = self.d_model // self.num_heads\n",
    "        self.wq, self.wk, self.wv, self.w = Dense(d_model), Dense(d_model), Dense(d_model), Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        # 将最后一维分割为num_heads * self.depth\n",
    "        # [bs, seq, dim] -> [bs, seq, heads, subdim] -> [bs, heads, seq, subdim]\n",
    "        batch_size = x.shape[0]\n",
    "        x = tf.reshape(x, [batch_size, -1, self.num_heads, self.depth])\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v):\n",
    "        self.q = self.wq(q)\n",
    "        self.k = self.wk(k)\n",
    "        self.v = self.wv(v)\n",
    "        self.q = self.split_heads(self.q)\n",
    "        self.k = self.split_heads(self.k)\n",
    "        self.v = self.split_heads(self.v)\n",
    "        splited_output = scaled_dot_product_attention(self.q, self.k, self.v)\n",
    "        batch_size = q.shape[0]\n",
    "        multi_head_output = tf.transpose(splited_output, [0, 2, 1, 3]) # [bs, head, seq, subdim] -> [bs, seq, head, subdim]\n",
    "        concated_output = tf.reshape(multi_head_output, [batch_size, -1, self.d_model])\n",
    "        output = self.w(concated_output)\n",
    "        return output\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 10, 512), dtype=float32, numpy=\n",
       "array([[[-0.00364481, -0.18792947,  0.24575943, ..., -0.3051786 ,\n",
       "          0.05009094,  0.49072775],\n",
       "        [-0.3751133 , -0.16968033,  0.63417685, ..., -0.42369777,\n",
       "         -0.28306895,  0.23018757],\n",
       "        [ 0.02423792, -0.34091505,  0.26673567, ..., -0.394813  ,\n",
       "          0.3934161 ,  0.4681937 ],\n",
       "        ...,\n",
       "        [ 0.51455647, -0.3019976 ,  0.1784493 , ...,  0.05739626,\n",
       "          0.11175066,  0.2889152 ],\n",
       "        [-0.5169923 , -0.14276664, -0.5098453 , ..., -0.6805406 ,\n",
       "         -0.2078146 , -0.01302366],\n",
       "        [-0.26270923, -0.25190753,  0.17749542, ..., -0.2725378 ,\n",
       "         -0.27025217,  0.3742838 ]],\n",
       "\n",
       "       [[ 0.12412558,  0.26256436, -0.33459312, ..., -0.4053634 ,\n",
       "         -0.04337247,  0.7351228 ],\n",
       "        [-0.1413969 ,  0.05620397,  0.17358118, ..., -0.70191425,\n",
       "          0.270257  ,  0.62721497],\n",
       "        [ 0.32794735,  0.27703953,  0.39264506, ..., -0.35090202,\n",
       "         -0.14802665,  0.248271  ],\n",
       "        ...,\n",
       "        [ 0.38383928, -0.15732035,  0.13743092, ...,  0.06156722,\n",
       "          0.5683928 , -0.25864133],\n",
       "        [ 0.05029716,  0.36939204,  0.4490947 , ..., -0.11865745,\n",
       "          0.01967864,  0.36865813],\n",
       "        [ 0.01272115,  0.05779031, -0.0209639 , ..., -0.47876608,\n",
       "         -0.08225122,  0.49384588]],\n",
       "\n",
       "       [[-0.503351  , -0.35940933, -0.23496844, ...,  0.4671246 ,\n",
       "         -0.593624  ,  0.28527302],\n",
       "        [-0.1808623 , -0.0330575 , -0.4051314 , ...,  0.34019294,\n",
       "          0.06459923,  0.28035083],\n",
       "        [-0.4525445 , -0.4493922 , -0.62768054, ..., -0.130022  ,\n",
       "         -0.6661194 ,  0.04533727],\n",
       "        ...,\n",
       "        [-0.34609586, -0.24984974, -0.3258748 , ...,  0.52646023,\n",
       "         -0.00428515,  0.17587055],\n",
       "        [-0.6363819 , -0.01189379, -0.30971903, ...,  0.1480475 ,\n",
       "         -0.00957613,  0.37399137],\n",
       "        [-0.22702654, -0.21919757,  0.07748921, ...,  0.4959098 ,\n",
       "         -0.27563506,  0.17871302]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[-0.6714608 ,  0.12763321, -0.08080286, ..., -0.12118331,\n",
       "         -0.12735   ,  0.40236762],\n",
       "        [ 0.07053854,  0.38417447, -0.2073969 , ...,  0.26611078,\n",
       "         -0.07705362,  0.61498284],\n",
       "        [-0.4732714 , -0.31712455, -0.03453779, ...,  0.1946142 ,\n",
       "         -0.1799259 ,  0.45621353],\n",
       "        ...,\n",
       "        [-0.22282621, -0.04717923, -0.35267925, ..., -0.07940777,\n",
       "         -0.11125109,  0.6673866 ],\n",
       "        [-0.34566423,  0.08838007, -0.2090856 , ...,  0.46289504,\n",
       "         -0.08036974,  0.5891773 ],\n",
       "        [-0.13081098, -0.21922688, -0.37849236, ..., -0.13892269,\n",
       "         -0.14961636,  0.25085777]],\n",
       "\n",
       "       [[-0.46741527, -0.30525082,  0.32010096, ..., -0.28166604,\n",
       "          0.15861115,  0.5238327 ],\n",
       "        [ 0.052461  , -0.38846844,  0.26621258, ..., -0.36047444,\n",
       "          0.3533825 ,  0.52620816],\n",
       "        [-0.26398805, -0.37852234,  0.48234698, ...,  0.12079997,\n",
       "          0.2427834 ,  0.52857697],\n",
       "        ...,\n",
       "        [-0.68766946, -0.20025945,  0.00826071, ..., -0.07784355,\n",
       "          0.2619202 ,  0.34093383],\n",
       "        [ 0.03603871, -0.15378678, -0.02616078, ..., -0.12968221,\n",
       "          0.22881691, -0.12140495],\n",
       "        [ 0.01474026, -0.5410229 ,  0.00561219, ..., -0.18057989,\n",
       "          0.10609837,  0.11953622]],\n",
       "\n",
       "       [[-0.21207091,  0.82932246,  0.33062223, ...,  0.06403033,\n",
       "          0.40975446,  0.3879513 ],\n",
       "        [-0.33912194,  0.24063158,  0.2157445 , ..., -0.03998975,\n",
       "          0.06312119,  0.19958854],\n",
       "        [-0.22194858,  0.4945398 ,  0.40409416, ...,  0.28113115,\n",
       "          0.2689353 ,  0.38136995],\n",
       "        ...,\n",
       "        [-0.49500772,  0.38329765,  0.26635012, ..., -0.3034313 ,\n",
       "          0.49540675,  0.55115384],\n",
       "        [-0.49455005,  0.512347  ,  0.34083158, ...,  0.05916369,\n",
       "          0.6206545 ,  0.42508346],\n",
       "        [-0.30663353,  0.39573818,  0.29716367, ...,  0.31848583,\n",
       "          0.40250373,  0.1741069 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
    "mha(q,k,v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
